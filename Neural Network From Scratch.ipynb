{"cells":[{"cell_type":"markdown","source":"### This code has been inspired by : http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n\n### LAB - Neural Network From Scratch \n#### Numan SAHNOU & Matthieu ECCHER","metadata":{"tags":[],"cell_id":"00000-9e347e89-a7bd-4f10-8cd7-d7f2dc7aa69b","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"We defined a `Multiply` class, it returns the operation __X * W__ (= we called this operation __Z__) in \"forward\" mode and return __dW__ and __dX__ (prefix 'd' means previous values) in \"backward\" mode","metadata":{"tags":[],"cell_id":"00001-82b84aec-efe3-4ec0-b05a-69a29e07f6de","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-17a10fc7-b769-4705-823a-df07f2f8b229","deepnote_to_be_reexecuted":false,"source_hash":"e1d2027","execution_millis":2,"execution_start":1610053501043,"deepnote_cell_type":"code"},"source":"import numpy as np\n\nclass Multiply:\n    def forward(self,W, X):\n        return np.dot(X, W)\n\n    def backward(self, W, X, dZ):\n        dW = np.dot(np.transpose(X), dZ)\n        dX = np.dot(dZ, np.transpose(W))\n        return dW, dX","execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Here we defined a `Add` class, it returns the operation __X + b__ (= we called this operation __Z_biased__) in \"forward\" mode and return __db__ and __dX__ in \"backward\" mode","metadata":{"tags":[],"cell_id":"00003-c3ab9782-e7d0-4f5c-a84e-b8d1396fdca6","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-40460c07-ce15-4c71-b016-e2c3bc2a4446","deepnote_to_be_reexecuted":false,"source_hash":"e6ad3c33","execution_millis":0,"execution_start":1610053501074,"deepnote_cell_type":"code"},"source":"class Add:\n    def forward(self, X, b):\n        return X + b\n\n    def backward(self, X, b, dZ):\n        dX = dZ * np.ones_like(X)\n        db = np.dot(np.ones((1, dZ.shape[0]), dtype=np.float64), dZ)\n        return db, dX","execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"__Sigmoid__ function, in \"forward\" we simply apply Sigmoid to Z, in \"backward\" we retrieve __dZ_biased__","metadata":{"tags":[],"cell_id":"00005-1302a958-e8bb-4e46-9d42-02eb1edf304c","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-f261afe0-780a-4811-863e-3cdf825ab9b6","deepnote_to_be_reexecuted":false,"source_hash":"13279eb4","execution_millis":1,"execution_start":1610053501074,"deepnote_cell_type":"code"},"source":"class Sigmoid:\n    def forward(self, Z):\n        return 1 / (1 + np.exp(-Z))\n\n    def backward(self, Z, top_diff):\n        output = self.forward(Z)\n        return (1 - output) * output * top_diff","execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"__Softmax__ used for the ouput. ","metadata":{"tags":[],"cell_id":"00007-e57e1646-22c0-427a-93e6-40ad2bcb06fd","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-b71cf0e5-229a-421d-a8f4-a00e1451a6e0","deepnote_to_be_reexecuted":false,"source_hash":"aeb062fe","execution_millis":0,"execution_start":1610053501075,"deepnote_cell_type":"code"},"source":"def softmax(X):\n    return np.exp(X) / np.sum(np.exp(X), axis=1, keepdims=True)","execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Model class\n\nWe can implement out neural network by a class `Model` and initialize the parameters in the `__init__` function (Weight and Bias)\n\nFirst we implement the loss function (`calculate_loss`). It is just a forward propagation computation of our neural network. We use this to evaluate how well our model is doing\n<br>We also implement `predict` function to calculate the output of the network. It does forward propagation and returns the class with the highest probability.<br>\nFinally, we defined the function `train` to __train__ our Neural Network. It implements batch gradient descent using the __backpropagation algorithms__ ","metadata":{"tags":[],"cell_id":"00002-fa35cb5d-957f-459c-813c-04db1ebaa23c","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-aa906f36-e786-477b-afd5-462d83692c96","deepnote_to_be_reexecuted":false,"source_hash":"fa5f2df4","execution_millis":4,"execution_start":1610053501076,"deepnote_cell_type":"code"},"source":"class Model:\n    def __init__(self, layers_dim):\n        self.b = []\n        self.W = []\n        for i in range(len(layers_dim)-1):\n            self.W.append(np.random.randn(layers_dim[i], layers_dim[i+1]) / np.sqrt(layers_dim[i]))\n            self.b.append(np.random.randn(layers_dim[i+1]).reshape(1, layers_dim[i+1]))\n    \n    def diff(self, X, y):\n        num_examples = X.shape[0]\n        probs = softmax(X)\n        probs[range(num_examples), y] -= 1\n        return probs\n\n    def calculate_loss(self, X2, y):\n        mul = Multiply()\n        add = Add()\n        sigmoid = Sigmoid()\n        \n        X = X2\n        for i in range(len(self.W)):\n            Z = mul.forward(self.W[i], X)\n            Z_biased = add.forward(Z, self.b[i])\n            X = sigmoid.forward(Z_biased)\n\n        num_examples = X.shape[0]\n        probs = softmax(X)\n        correct_logprobs = -np.log(probs[range(num_examples), y])\n        data_loss = np.sum(correct_logprobs)\n        return 1/num_examples * data_loss\n\n    #Return the class with the highest probability\n    def predict(self, X2):\n        mul = Multiply()\n        add = Add()\n        sigmoid = Sigmoid()\n\n        X = X2\n        for i in range(len(self.W)):\n            Z = mul.forward(self.W[i], X)\n            Z_biased = add.forward(Z, self.b[i])\n            X = sigmoid.forward(Z_biased)\n\n        probs = softmax(X)\n        return np.argmax(probs, axis=1)\n\n    def train(self, X2, y, iterations, alpha, delta, print_loss=False):\n        mul = Multiply()\n        add = Add()\n        sigmoid = Sigmoid()\n\n        for epoch in range(iterations):\n            # Forward propagation\n            X = X2\n            forward = [(None, None, X)]\n            for i in range(len(self.W)):\n                Z = mul.forward(self.W[i], X)\n                Z_biased = add.forward(Z, self.b[i])\n                X = sigmoid.forward(Z_biased)\n                forward.append((Z, Z_biased, X))\n\n            # Back propagation\n            dsigmoid = self.diff(forward[len(forward)-1][2], y)\n            for i in range(len(forward)-1, 0, -1):\n                dZ_biased = sigmoid.backward(forward[i][1], dsigmoid)\n                db, dZ = add.backward(forward[i][0], self.b[i-1], dZ_biased)\n                dW, dsigmoid = mul.backward(self.W[i-1], forward[i-1][2], dZ)\n                # Add regularization terms\n                dW += delta * self.W[i-1]\n                # Gradient descent parameter update\n                self.b[i-1] += -alpha * db\n                self.W[i-1] += -alpha * dW\n\n            if print_loss and epoch % 100 == 0:\n                print(\"Loss after iteration %i: %f\" %(epoch, self.calculate_loss(X2, y)))","execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Execution of the model\n\nHere we will execute the model with an input layer of dimention __400__, an hidden layer of dimention __150__ and an ouput of dimension __10__","metadata":{"tags":[],"cell_id":"00004-0da23316-3975-43b7-9528-fd0b834a07de","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-01b82975-5ea3-47e4-85f8-d534c6b2aab2","deepnote_to_be_reexecuted":false,"source_hash":"c105cff5","execution_millis":127516,"execution_start":1610053501084,"deepnote_cell_type":"code"},"source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn.datasets\nimport sklearn.linear_model\nfrom sklearn.model_selection import train_test_split \n\nX = np.loadtxt(\"features.txt\", delimiter = \",\",dtype=float)\ny = np.loadtxt(\"labels.txt\", delimiter = \",\",dtype=int)\n\ny[y == 10] = 0\n\nlayers_dim = [400, 150, 10]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n\nmodel = Model(layers_dim)\nmodel.train(X_train, y_train, iterations=1000, alpha=0.001, delta=0.0001, print_loss=True)\ny_predicted = model.predict(X_test)\n    \ndef score_from_scratch(y_predicted, y_actual):\n    cpt=0\n    for i in range(len(y_predicted)):\n        if(y_predicted[i] == y_actual[i]):\n            cpt+=1\n            \n    return cpt/len(y_predicted)\n\nprint(\"Accuracy of the model from scratch : \", score_from_scratch(y_predicted, y_test))","execution_count":6,"outputs":[{"name":"stdout","text":"Loss after iteration 0: 2.295576\nLoss after iteration 100: 1.677630\nLoss after iteration 200: 1.617462\nLoss after iteration 300: 1.586783\nLoss after iteration 400: 1.569411\nLoss after iteration 500: 1.558131\nLoss after iteration 600: 1.549886\nLoss after iteration 700: 1.543463\nLoss after iteration 800: 1.538243\nLoss after iteration 900: 1.533867\nAccuracy of the model from scratch :  0.919\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Confusion Matrix of the model from scratch","metadata":{"tags":[],"cell_id":"00013-58cef03a-9d9d-40b8-aa4d-5a992b8ee93f","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00006-b5be6bf8-55fa-443b-84fe-34b408e356d8","deepnote_to_be_reexecuted":false,"source_hash":"38972766","execution_millis":87,"execution_start":1610053628605,"deepnote_cell_type":"code"},"source":"from sklearn.metrics import confusion_matrix\nimport pandas as pd\n\ny_predicted = pd.get_dummies(y_predicted, columns = [0])\ny_test = pd.get_dummies(y_test, columns = [0])\ny_train = pd.get_dummies(y_train, columns = [0])\n\nmatrix = confusion_matrix(\n    y_test.to_numpy().argmax(axis=1), y_predicted.to_numpy().argmax(axis=1))\n\nmatrix","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"array([[114,   0,   1,   0,   0,   1,   1,   0,   1,   0],\n       [  0, 100,   1,   0,   0,   0,   0,   0,   1,   0],\n       [  3,   1,  89,   1,   3,   0,   0,   2,   3,   0],\n       [  0,   1,   6,  88,   0,   1,   0,   0,   1,   0],\n       [  1,   0,   0,   0,  95,   0,   0,   0,   1,   6],\n       [  1,   2,   1,   4,   1,  89,   2,   0,   5,   0],\n       [  0,   0,   1,   0,   0,   2,  96,   0,   0,   0],\n       [  0,   2,   0,   0,   4,   0,   0,  96,   0,   2],\n       [  0,   2,   4,   2,   1,   1,   2,   0,  70,   0],\n       [  1,   0,   0,   0,   1,   0,   1,   2,   1,  82]])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Comparison with tensorflow ","metadata":{"tags":[],"cell_id":"00005-20f145cd-1cde-41b2-b193-a9dfbefd8841","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-d1c151aa-e4ec-4d58-bdf2-d6425651f8c7","deepnote_to_be_reexecuted":false,"source_hash":"6001b7a9","execution_millis":7692,"execution_start":1610054018165,"deepnote_cell_type":"code"},"source":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom keras.optimizers import SGD\n\nmodel = Sequential()\n\nmodel.add(Dense(units=400, activation='sigmoid', input_shape=(400,)))\nmodel.add(Dense(units=150, activation='sigmoid'))\nmodel.add(Dense(units=10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n\nmodel.fit(X_train, y_train, batch_size = 400, epochs = 100, verbose=1)\nprint(\"Model prediction classes :\\n \", model.predict_classes(X_test))\n\nscore, acc = model.evaluate(X_test, y_test,batch_size=300)\n\nprint(\"\\n Accuracy of Tensorflow model : \", acc)","execution_count":14,"outputs":[{"name":"stdout","text":"Epoch 1/100\n10/10 [==============================] - 0s 5ms/step - loss: 2.4293 - accuracy: 0.1336\nEpoch 2/100\n10/10 [==============================] - 0s 4ms/step - loss: 2.1833 - accuracy: 0.2023\nEpoch 3/100\n10/10 [==============================] - 0s 9ms/step - loss: 2.0032 - accuracy: 0.5243\nEpoch 4/100\n10/10 [==============================] - 0s 9ms/step - loss: 1.7724 - accuracy: 0.7350\nEpoch 5/100\n10/10 [==============================] - 0s 9ms/step - loss: 1.5167 - accuracy: 0.7176\nEpoch 6/100\n10/10 [==============================] - 0s 4ms/step - loss: 1.2723 - accuracy: 0.7551\nEpoch 7/100\n10/10 [==============================] - 0s 9ms/step - loss: 1.0731 - accuracy: 0.7908\nEpoch 8/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.9297 - accuracy: 0.8083\nEpoch 9/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.8070 - accuracy: 0.8254\nEpoch 10/100\n10/10 [==============================] - 0s 3ms/step - loss: 0.7144 - accuracy: 0.8340\nEpoch 11/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.6449 - accuracy: 0.8535\nEpoch 12/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.5587 - accuracy: 0.8743\nEpoch 13/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.5327 - accuracy: 0.8692\nEpoch 14/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.4912 - accuracy: 0.8856\nEpoch 15/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.4508 - accuracy: 0.8873\nEpoch 16/100\n10/10 [==============================] - 0s 3ms/step - loss: 0.4254 - accuracy: 0.8972\nEpoch 17/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.3994 - accuracy: 0.8980\nEpoch 18/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.4045 - accuracy: 0.8917\nEpoch 19/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.3552 - accuracy: 0.9094\nEpoch 20/100\n10/10 [==============================] - 0s 5ms/step - loss: 0.3664 - accuracy: 0.9026\nEpoch 21/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.3343 - accuracy: 0.9173\nEpoch 22/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.3250 - accuracy: 0.9183\nEpoch 23/100\n10/10 [==============================] - 0s 3ms/step - loss: 0.3181 - accuracy: 0.9213\nEpoch 24/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.3032 - accuracy: 0.9255\nEpoch 25/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.3068 - accuracy: 0.9230\nEpoch 26/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.2954 - accuracy: 0.9267\nEpoch 27/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.3055 - accuracy: 0.9252\nEpoch 28/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.2754 - accuracy: 0.9310\nEpoch 29/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.2531 - accuracy: 0.9349\nEpoch 30/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.2719 - accuracy: 0.9312\nEpoch 31/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.2595 - accuracy: 0.9342\nEpoch 32/100\n10/10 [==============================] - 0s 10ms/step - loss: 0.2561 - accuracy: 0.9303\nEpoch 33/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.2400 - accuracy: 0.9415\nEpoch 34/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.2385 - accuracy: 0.9368\nEpoch 35/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.2368 - accuracy: 0.9389\nEpoch 36/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.2255 - accuracy: 0.9466\nEpoch 37/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.2287 - accuracy: 0.9440\nEpoch 38/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.2235 - accuracy: 0.9419\nEpoch 39/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.2185 - accuracy: 0.9442\nEpoch 40/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.2305 - accuracy: 0.9422\nEpoch 41/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.2181 - accuracy: 0.9441\nEpoch 42/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.2002 - accuracy: 0.9511\nEpoch 43/100\n10/10 [==============================] - 0s 3ms/step - loss: 0.1987 - accuracy: 0.9487\nEpoch 44/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.2078 - accuracy: 0.9474\nEpoch 45/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1890 - accuracy: 0.9503\nEpoch 46/100\n10/10 [==============================] - 0s 3ms/step - loss: 0.1885 - accuracy: 0.9518\nEpoch 47/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.1913 - accuracy: 0.9512\nEpoch 48/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1779 - accuracy: 0.9566\nEpoch 49/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1698 - accuracy: 0.9556\nEpoch 50/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.1700 - accuracy: 0.9573\nEpoch 51/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1733 - accuracy: 0.9542\nEpoch 52/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.1651 - accuracy: 0.9580\nEpoch 53/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1714 - accuracy: 0.9576\nEpoch 54/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.1728 - accuracy: 0.9500\nEpoch 55/100\n10/10 [==============================] - 0s 10ms/step - loss: 0.1677 - accuracy: 0.9564\nEpoch 56/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1484 - accuracy: 0.9652\nEpoch 57/100\n10/10 [==============================] - 0s 3ms/step - loss: 0.1547 - accuracy: 0.9634\nEpoch 58/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.1545 - accuracy: 0.9618\nEpoch 59/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1600 - accuracy: 0.9595\nEpoch 60/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1608 - accuracy: 0.9586\nEpoch 61/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.1401 - accuracy: 0.9652\nEpoch 62/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1416 - accuracy: 0.9622\nEpoch 63/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1363 - accuracy: 0.9634\nEpoch 64/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.1367 - accuracy: 0.9640\nEpoch 65/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.1382 - accuracy: 0.9639\nEpoch 66/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1461 - accuracy: 0.9631\nEpoch 67/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1319 - accuracy: 0.9686\nEpoch 68/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.1221 - accuracy: 0.9700\nEpoch 69/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1192 - accuracy: 0.9720\nEpoch 70/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1263 - accuracy: 0.9696\nEpoch 71/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.1176 - accuracy: 0.9715\nEpoch 72/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.1190 - accuracy: 0.9753\nEpoch 73/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1157 - accuracy: 0.9737\nEpoch 74/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1188 - accuracy: 0.9709\nEpoch 75/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1082 - accuracy: 0.9759\nEpoch 76/100\n10/10 [==============================] - 0s 3ms/step - loss: 0.1137 - accuracy: 0.9725\nEpoch 77/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1145 - accuracy: 0.9732\nEpoch 78/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1104 - accuracy: 0.9734\nEpoch 79/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.0973 - accuracy: 0.9770\nEpoch 80/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0911 - accuracy: 0.9793\nEpoch 81/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.0999 - accuracy: 0.9781\nEpoch 82/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1099 - accuracy: 0.9783\nEpoch 83/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.0918 - accuracy: 0.9804\nEpoch 84/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.0950 - accuracy: 0.9815\nEpoch 85/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1027 - accuracy: 0.9779\nEpoch 86/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.1018 - accuracy: 0.9760\nEpoch 87/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.0918 - accuracy: 0.9795\nEpoch 88/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.0908 - accuracy: 0.9809\nEpoch 89/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.0833 - accuracy: 0.9826\nEpoch 90/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.0804 - accuracy: 0.9835\nEpoch 91/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.0861 - accuracy: 0.9827\nEpoch 92/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.0943 - accuracy: 0.9796\nEpoch 93/100\n10/10 [==============================] - 0s 3ms/step - loss: 0.0786 - accuracy: 0.9841\nEpoch 94/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0844 - accuracy: 0.9813\nEpoch 95/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.0741 - accuracy: 0.9855\nEpoch 96/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.0726 - accuracy: 0.9864\nEpoch 97/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.0702 - accuracy: 0.9876\nEpoch 98/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.0707 - accuracy: 0.9888\nEpoch 99/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.0734 - accuracy: 0.9859\nEpoch 100/100\n10/10 [==============================] - 0s 4ms/step - loss: 0.0636 - accuracy: 0.9873\nModel prediction classes :\n  [2 3 4 8 3 9 8 2 4 7 7 8 6 2 4 3 3 5 4 4 4 2 5 9 9 1 6 5 4 7 9 8 1 6 3 0 2\n 4 7 8 7 4 6 8 0 2 6 6 4 1 5 6 7 9 2 9 6 7 1 6 4 1 2 3 4 0 1 4 4 0 3 8 6 3\n 5 6 4 9 8 5 5 1 7 0 5 3 4 2 0 1 4 6 0 1 4 0 4 3 6 4 8 5 5 3 1 6 8 5 6 1 3\n 3 2 9 4 0 7 4 7 6 8 7 5 8 9 5 6 9 0 8 7 2 6 1 1 5 0 2 5 4 6 5 7 1 8 6 0 9\n 9 1 1 8 6 6 2 3 2 4 2 5 8 5 2 5 5 8 9 1 5 4 6 3 7 5 0 6 4 1 6 9 8 2 2 0 8\n 0 5 9 1 7 1 7 6 7 9 5 8 9 6 5 7 8 9 6 3 3 2 1 4 8 1 4 7 7 7 2 7 5 5 4 7 4\n 6 0 6 7 1 0 4 9 6 2 0 4 0 4 1 0 5 3 0 6 1 4 5 1 5 8 4 2 2 3 9 2 9 8 1 4 9\n 7 8 6 3 7 7 4 5 4 2 6 2 0 0 5 8 5 2 4 1 2 1 2 1 0 2 4 0 9 8 5 5 9 6 5 3 6\n 4 0 0 0 1 5 4 3 0 6 3 4 9 4 6 3 3 4 2 8 1 0 4 6 6 4 6 1 1 0 2 1 4 0 1 4 9\n 1 0 4 7 0 0 7 6 1 0 0 2 1 6 0 4 7 3 7 4 0 2 0 1 8 4 0 0 2 4 0 2 7 0 4 4 2\n 2 0 5 1 4 5 9 6 3 7 9 4 7 7 5 7 3 8 7 5 6 6 8 0 6 0 4 8 0 3 2 6 2 0 0 7 7\n 7 0 5 4 9 4 5 6 4 7 7 9 3 2 5 2 5 5 7 9 2 0 1 3 3 4 8 3 6 2 0 9 5 1 5 3 0\n 1 3 9 5 1 2 5 0 1 1 2 4 5 1 0 9 1 9 1 3 2 0 2 8 9 5 0 7 1 9 7 3 1 5 2 8 9\n 7 2 2 1 3 8 9 9 8 5 4 0 7 6 3 1 6 6 2 9 8 5 0 9 0 5 3 2 2 3 8 2 0 0 5 7 4\n 1 3 1 4 5 1 8 1 0 7 3 8 2 1 9 2 0 1 9 3 9 7 3 0 7 0 8 9 3 9 7 5 3 1 1 5 5\n 0 5 8 5 1 7 4 7 9 0 6 3 2 3 3 9 5 8 8 7 7 2 2 2 8 2 1 1 7 6 0 9 0 3 8 6 4\n 5 7 0 6 6 2 8 6 5 6 7 0 0 1 7 3 4 6 3 2 7 0 6 5 1 8 7 3 5 2 5 4 1 8 9 9 9\n 9 4 3 1 5 3 5 5 3 2 8 1 3 0 2 0 6 1 3 4 3 3 9 3 0 4 1 7 2 7 2 9 5 9 5 6 0\n 2 0 2 9 6 5 0 5 3 5 9 1 8 9 8 9 6 7 4 7 8 1 1 3 1 1 8 5 4 7 3 3 8 6 3 8 3\n 8 6 5 9 7 0 7 7 7 7 4 5 6 8 7 4 9 4 4 9 5 0 7 0 2 3 4 4 9 1 3 5 0 7 1 6 9\n 6 8 9 8 0 1 5 0 8 7 4 6 4 6 9 6 7 6 4 8 3 9 8 9 5 4 2 7 9 1 2 1 0 1 5 7 8\n 3 6 5 4 1 3 7 3 9 7 4 2 3 2 4 0 4 3 1 5 2 0 0 2 5 1 8 5 0 1 2 3 2 3 6 7 2\n 8 1 6 9 9 2 0 9 5 1 7 9 4 1 9 5 8 0 8 7 1 2 5 8 2 3 8 8 0 7 6 0 4 8 4 1 9\n 4 0 3 1 0 6 7 1 5 9 7 2 0 0 6 0 4 0 1 9 3 7 0 2 7 4 3 8 5 6 2 5 6 4 6 7 4\n 6 9 0 2 8 0 5 1 3 1 2 3 1 6 1 6 2 5 1 5 0 1 7 0 0 3 0 7 6 7 3 2 0 5 3 3 6\n 9 0 5 2 0 2 6 7 7 8 7 4 3 8 5 6 2 2 9 8 5 8 3 2 9 7 2 9 5 4 3 5 7 5 0 8 9\n 9 1 8 8 6 2 9 0 3 9 4 7 6 7 8 9 4 5 4 1 2 6 3 4 4 0 3 5 0 6 6 1 6 3 6 2 1\n 6]\n/opt/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n  warnings.warn('`model.predict_classes()` is deprecated and '\n4/4 [==============================] - 0s 2ms/step - loss: 0.1789 - accuracy: 0.9350\n\n Accuracy of Tensorflow model :  0.9350000023841858\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Confusion Matrix Tensorflow Keras","metadata":{"tags":[],"cell_id":"00017-a924f1f8-99c5-4bce-9d54-526b6d7203e1","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00017-8472bb6c-6cfa-421b-bb79-1a8174494b8d","deepnote_to_be_reexecuted":false,"source_hash":"a4acee9c","execution_millis":14,"execution_start":1610054033617,"deepnote_cell_type":"code"},"source":"from sklearn.metrics import confusion_matrix\n\ny_pred = model.predict_classes(X_test)\n\ny_pred = pd.get_dummies(y_pred, columns = [0])\n\nmatrix_keras = confusion_matrix(\n    y_test.to_numpy().argmax(axis=1), y_pred.to_numpy().argmax(axis=1))\n\nmatrix_keras","execution_count":15,"outputs":[{"name":"stderr","text":"/opt/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n  warnings.warn('`model.predict_classes()` is deprecated and '\n","output_type":"stream"},{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"array([[113,   0,   0,   0,   0,   2,   2,   0,   1,   0],\n       [  0, 100,   1,   0,   1,   0,   0,   0,   0,   0],\n       [  2,   0,  94,   1,   2,   0,   0,   1,   2,   0],\n       [  0,   0,   2,  87,   0,   6,   0,   0,   1,   1],\n       [  0,   0,   0,   0,  98,   0,   0,   0,   1,   4],\n       [  0,   1,   0,   3,   0,  97,   1,   0,   3,   0],\n       [  0,   0,   1,   0,   0,   2,  96,   0,   0,   0],\n       [  0,   1,   0,   1,   1,   0,   0,  96,   0,   5],\n       [  0,   3,   1,   3,   1,   0,   0,   0,  74,   0],\n       [  1,   0,   1,   0,   2,   0,   0,   3,   1,  80]])"},"metadata":{}}]},{"cell_type":"markdown","source":"### Comparison of the two confusion matrix","metadata":{"tags":[],"cell_id":"00018-56858207-8506-460d-8ad7-5083fb38037b","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00018-96c666a3-e30c-4676-a276-c67b62209a92","deepnote_to_be_reexecuted":false,"source_hash":"6132ebb0","execution_millis":3,"execution_start":1610054036697,"deepnote_cell_type":"code"},"source":"print(\"Confusion Matrix model from scratch \\n\",matrix, \"\\n Confusion Matrix keras model \\n\", matrix_keras)","execution_count":16,"outputs":[{"name":"stdout","text":"Confusion Matrix model from scratch \n [[114   0   1   0   0   1   1   0   1   0]\n [  0 100   1   0   0   0   0   0   1   0]\n [  3   1  89   1   3   0   0   2   3   0]\n [  0   1   6  88   0   1   0   0   1   0]\n [  1   0   0   0  95   0   0   0   1   6]\n [  1   2   1   4   1  89   2   0   5   0]\n [  0   0   1   0   0   2  96   0   0   0]\n [  0   2   0   0   4   0   0  96   0   2]\n [  0   2   4   2   1   1   2   0  70   0]\n [  1   0   0   0   1   0   1   2   1  82]] \n Confusion Matrix keras model \n [[113   0   0   0   0   2   2   0   1   0]\n [  0 100   1   0   1   0   0   0   0   0]\n [  2   0  94   1   2   0   0   1   2   0]\n [  0   0   2  87   0   6   0   0   1   1]\n [  0   0   0   0  98   0   0   0   1   4]\n [  0   1   0   3   0  97   1   0   3   0]\n [  0   0   1   0   0   2  96   0   0   0]\n [  0   1   0   1   1   0   0  96   0   5]\n [  0   3   1   3   1   0   0   0  74   0]\n [  1   0   1   0   2   0   0   3   1  80]]\n","output_type":"stream"}]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"8f36f7f0-073c-4d2e-a123-373e59ff5010","deepnote_execution_queue":[]}}